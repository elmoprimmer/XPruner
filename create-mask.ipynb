{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a850c1f-33f8-4d8e-9404-ef33777795f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import imagenet as imgnt\n",
    "from utils.evaluate import evaluate, evaluate_wrapped, Accuracy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "import random\n",
    "import inspect\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "\n",
    "from models.deit import MaskedDeiT as MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "897f4cb8-0962-4167-9cd0-f317eb06131b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageNetDataset\n",
       "    Number of datapoints: 50000\n",
       "    Root location: /scratch_shared/primmere/ILSVRC/Data/CLS-LOC/val\n",
       "    Compose(\n",
       "    ToTensor()\n",
       "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"/scratch_shared/primmere/ILSVRC/Data/CLS-LOC\"\n",
    "imagenet = imgnt.ImageNet(dataset_path, 1)\n",
    "val = imagenet.get_valid_set()\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = timm.create_model('deit_tiny_patch16_224.fb_in1k', pretrained=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "next(model.parameters()).is_cuda \n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b0518c-d361-43c7-b2b8-13b433b103ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#indices = random.sample(range(50_000),1000)\n",
    "indices = imgnt.get_sample_indices_for_class(val, list(range(10)), 10_000, device)\n",
    "val_small = imgnt.ImageNetSubset(val,indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4be03a-10ee-4fea-8183-014745a363d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_small,4, pin_memory=True)\n",
    "img, label = next(iter(val_loader))\n",
    "img, label = img.to(device), label.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f368e012-d3c0-473c-9a70-f3b9d5585adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.cuda.LongTensor'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5161344a-a8c2-4c5a-be4d-44f102d3433e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc05e4-4c17-42c8-b164-a64263d2e98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f4b4a8-0bee-4397-83f9-a4160414609d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90929e17-04ac-48a0-a360-ea5e566bf8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   0, 391,   0], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model(img), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdcbd0f0-2615-4ca2-a38e-80c49e4a37a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 125/125 [00:03<00:00, 33.90batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47  0  0 ...  1  0  0]\n",
      " [ 0 45  0 ...  0  0  0]\n",
      " [ 0  0 31 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "0.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc = Accuracy()\n",
    "results = evaluate(model, val_loader, acc, device)\n",
    "print(results['confusion_matrix'])\n",
    "print(results['total_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3c862-67f4-411a-bbdb-e0bf5aaae452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "798827e9-607a-4e16-af5e-efa71ef02dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = len(model.blocks)\n",
    "print(L)\n",
    "d_l = model.blocks[0].attn.num_heads\n",
    "print(d_l)\n",
    "model.blocks[0].attn.head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47cdfc34-b3fb-461d-983f-aa837d999414",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41bb4650-0c70-4fae-8b27-08f2d646deea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "target = model.blocks[0].attn.qkv\n",
    "handle = target.register_forward_hook(\n",
    "    lambda m, inp, out: setattr(m, \"saved_input\", inp[0].detach())\n",
    ")\n",
    "\n",
    "pred = model(img)\n",
    "\n",
    "x = target.saved_input\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29d679ae-abb8-4819-9855-22ad5a088f65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m            Attention\n",
       "\u001b[0;31mString form:\u001b[0m    \n",
       "Attention(\n",
       "  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "  (q_norm): Identity()\n",
       "  (k_norm): Identity()\n",
       "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       ")\n",
       "\u001b[0;31mFile:\u001b[0m            ~/.conda/envs/dfr2/lib/python3.10/site-packages/timm/models/vision_transformer.py\n",
       "\u001b[0;31mSource:\u001b[0m         \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfused_attn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFinal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mnum_heads\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mqkv_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mqk_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mproj_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mattn_drop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mproj_drop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mnorm_layer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_heads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dim should be divisible by num_heads'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfused_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_fused_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqkv_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mqk_norm\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mqk_norm\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mqkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfused_attn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_drop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mClass docstring:\u001b[0m\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool\n",
       "\u001b[0;31mInit docstring:\u001b[0m  Initialize internal Module state, shared by both nn.Module and ScriptModule."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.blocks[0].attn??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6fc3fea-7a13-405e-8488-09a8b4fbcb85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedAttn(\n",
      "  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "tensor([[ 7.2938e+00,  1.3162e+00,  2.5841e+00,  ...,  1.7861e+00,\n",
      "         -5.9208e-01,  8.4107e-02],\n",
      "        [ 9.4730e+00,  2.1688e+00, -7.6427e-01,  ...,  2.9838e+00,\n",
      "          1.5964e+00, -1.3424e+00],\n",
      "        [ 6.0448e+00, -3.7651e-01,  1.1977e+00,  ...,  4.6082e-01,\n",
      "          6.4149e-01, -1.8328e-04],\n",
      "        [ 9.1301e+00,  1.2231e+00, -4.9734e-01,  ...,  3.2658e+00,\n",
      "          9.5074e-01, -9.3547e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 7.2938e+00,  1.3162e+00,  2.5840e+00,  ...,  1.7861e+00,\n",
      "         -5.9208e-01,  8.4104e-02],\n",
      "        [ 9.4730e+00,  2.1688e+00, -7.6427e-01,  ...,  2.9838e+00,\n",
      "          1.5964e+00, -1.3424e+00],\n",
      "        [ 6.0448e+00, -3.7651e-01,  1.1977e+00,  ...,  4.6081e-01,\n",
      "          6.4148e-01, -1.8060e-04],\n",
      "        [ 9.1301e+00,  1.2231e+00, -4.9734e-01,  ...,  3.2658e+00,\n",
      "          9.5074e-01, -9.3547e-01]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class MaskedAttn(nn.Module):\n",
    "    \"\"\"\n",
    "    Attn module with mask\n",
    "    \"\"\"\n",
    "    def __init__(self, attn: nn.Module, num_classes: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.qkv = attn.qkv\n",
    "        self.proj = attn.proj\n",
    "        self.attn_drop = attn.attn_drop\n",
    "        self.proj_drop = attn.proj_drop\n",
    "        self.scale = attn.scale\n",
    "        self.num_heads = attn.num_heads\n",
    "        self.head_dim = attn.head_dim\n",
    "        self.num_classes = 10\n",
    "        self.mask_attn = nn.Parameter(torch.ones(self.num_classes, self.num_heads, self.head_dim)) # (C, H, D)\n",
    "        out_proj, in_proj = self.proj.weight.shape\n",
    "        self.mask_proj = nn.Parameter(torch.ones(self.num_classes, out_proj, in_proj)) # (C, out_proj, in_proj)\n",
    "\n",
    "        for p in self.qkv.parameters(): p.requires_grad = False\n",
    "        for p in self.proj.parameters(): p.requires_grad = False\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y=None) -> torch.Tensor:\n",
    "        B, N, C = x.shape # B = batch, N = num tokens (cls+patches), C = embed dim (head dim)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)  # (B, H, N, D)\n",
    "\n",
    "        q = q * self.scale\n",
    "        a = q @ k.transpose(-2, -1)\n",
    "        a = a.softmax(dim=-1)\n",
    "        a = self.attn_drop(a)\n",
    "        o = a @ v\n",
    "\n",
    "        if y is not None:\n",
    "            M = self.mask_attn[y] # (B, H, D)\n",
    "            M = M.unsqueeze(2) # (B, H, 1, D)\n",
    "            M_proj = self.mask_proj[y]\n",
    "        else:\n",
    "            assert 1 == 2, \"todo\"\n",
    "            \n",
    "            #todo: should take avg over classes and multiply\n",
    "\n",
    "        o = o * M\n",
    "\n",
    "        x = o.transpose(1, 2).reshape(B, N, C)\n",
    "\n",
    "        W_proj = self.proj.weight.unsqueeze(0)*M_proj # B, out, in\n",
    "        x = torch.einsum('bni,boi->bno', x, W_proj) + self.proj.bias.unsqueeze(0).unsqueeze(0) # B, N, C\n",
    "        # einsum bc we have weights with batch dim, so need batch mat mul\n",
    "\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class MaskedMlp(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP block with mask\n",
    "    \"\"\"\n",
    "    def __init__(self, mlp: nn.Module, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = mlp.fc1\n",
    "        self.act = mlp.act\n",
    "        self.drop1 = mlp.drop1\n",
    "        self.norm = mlp.norm\n",
    "        self.fc2 = mlp.fc2\n",
    "        self.drop2 = mlp.drop2\n",
    "\n",
    "        for p in self.fc1.parameters(): p.requires_grad = False\n",
    "        for p in self.fc2.parameters(): p.requires_grad = False\n",
    "        \n",
    "        self.num_classes = 10\n",
    "\n",
    "        out1, in1 = self.fc1.weight.shape\n",
    "        out2, in2 = self.fc2.weight.shape\n",
    "\n",
    "        self.mask_fc1 = nn.Parameter(torch.ones(self.num_classes, out1, in1))\n",
    "        self.mask_fc2 = nn.Parameter(torch.ones(self.num_classes, out2, in2))\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y=None) -> torch.Tensor:\n",
    "        # x = B,N,C\n",
    "        # y = B\n",
    "        if y is not None:\n",
    "            M_fc1 = self.mask_fc1[y] #B, out, in\n",
    "            M_fc2 = self.mask_fc2[y] #B, out, in\n",
    "        else:\n",
    "            assert 1 == 2, \"todo for inference\"\n",
    "            \n",
    "        W1 = self.fc1.weight.unsqueeze(0) * M_fc1 #B, out1, in1 (unsqeeze to add batch dim to fc1 weights)\n",
    "        b1 = self.fc1.bias # out1\n",
    "        x = torch.einsum('bni,boi->bno', x, W1) + b1.unsqueeze(0).unsqueeze(0) #B, N, out1\n",
    "        # einsum bc we have weights with batch dim, so need batch mat mul\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.norm(x)\n",
    "        W2 = self.fc2.weight.unsqueeze(0) * M_fc2 #B, out2, in2 (=out1)\n",
    "        b2 = self.fc2.bias\n",
    "        x = torch.einsum('bni,boi->bno', x, W2) + b2.unsqueeze(0).unsqueeze(0)\n",
    "        x = self.drop2(x)\n",
    "        return x # B, N, C\n",
    "\n",
    "class MaskedDeiT(nn.Module):\n",
    "    \"\"\"\n",
    "    deit with masked attn\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_classes = model.num_classes\n",
    "\n",
    "        self.masked_attn = nn.ModuleList()\n",
    "        self.masked_mlp = nn.ModuleList()\n",
    "        for blk in self.model.blocks:\n",
    "            self.masked_attn.append(MaskedAttn(blk.attn, num_classes = self.num_classes))\n",
    "            self.masked_mlp.append(MaskedMlp(blk.mlp, num_classes = self.num_classes))\n",
    "            \n",
    "\n",
    "        # turn off original modules\n",
    "        for blk in self.model.blocks:\n",
    "            for p in blk.attn.parameters(): p.requires_grad = False\n",
    "            for p in blk.mlp.parameters(): p.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward_features(self, x, y=None):\n",
    "        B = x.shape[0] # batch size\n",
    "        x = self.model.patch_embed(x)\n",
    "        cls_tok = self.model.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tok, x), dim=1) \n",
    "        x = x + self.model.pos_embed\n",
    "        x = self.model.pos_drop(x)\n",
    "\n",
    "        \n",
    "        for i, blk in enumerate(self.model.blocks):\n",
    "            attn_out = self.masked_attn[i](blk.norm1(x), y)\n",
    "            x = x + blk.drop_path1(blk.ls1(attn_out))\n",
    "            \n",
    "            mlp_out = self.masked_mlp[i](blk.norm2(x), y)\n",
    "            x = x + blk.drop_path2(blk.ls2(mlp_out))\n",
    "            \n",
    "        \n",
    "        x = self.model.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.forward_features(x, y)\n",
    "        return self.model.head(x)\n",
    "    \n",
    "\n",
    "masked_attn = MaskedAttn(model.blocks[0].attn, 1000)\n",
    "masked_attn\n",
    "print(masked_attn)\n",
    "\n",
    "\n",
    "\n",
    "wrapped = MaskedDeiT(model)\n",
    "wrapped.to(device)\n",
    "wrapped.eval()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "print(wrapped(img,label))\n",
    "print(model(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7384eb5a-a17c-42db-a7de-1a64d34e4c92",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     out_masked \u001b[38;5;241m=\u001b[39m \u001b[43mmasked_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     out_ref    \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattn(x)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mallclose(out_masked, out_ref, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/dfr2/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dfr2/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 35\u001b[0m, in \u001b[0;36mMaskedAttn.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m o \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m v\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     M \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_attn\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# (B, H, D)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     M \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (B, H, 1, D)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     M_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_proj[y]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out_masked = masked_attn(x, label)\n",
    "    out_ref    = model.blocks[0].attn(x)\n",
    "\n",
    "print(torch.allclose(out_masked, out_ref, rtol=1e-5, atol=1e-6))\n",
    "print(\"max abs diff:\", (out_masked - out_ref).abs().max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5fa1a-1673-4bea-a0fb-d9f57d76020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c508df-248b-4934-9ee4-0e69992dc329",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = Accuracy()\n",
    "results = evaluate(model, val_loader, acc, device)\n",
    "print(results['confusion_matrix'])\n",
    "print(results['total_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c151873-35bb-43e3-a9fd-437e0cc62ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = Accuracy()\n",
    "results = evaluate_wrapped(wrapped, val_loader, acc, device)\n",
    "print(results['confusion_matrix'])\n",
    "print(results['total_accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a0491e-6013-4254-9d98-a7325efcb3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped.to(device)\n",
    "wrapped(img,y=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf25e667-50b8-4c6d-903a-5cb057846ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f580e8bf-0332-491b-aa20-adf5e9fdaf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped2 = MD(model)\n",
    "wrapped2.to(device)\n",
    "wrapped2.eval()\n",
    "wrapped2(img,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c43d4-a583-497f-98df-a9c899528c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask_params = [m.mask for m in wrapped2.masked_attn]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfr2",
   "language": "python",
   "name": "dfr2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
