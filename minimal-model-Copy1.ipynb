{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a850c1f-33f8-4d8e-9404-ef33777795f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import imagenet as imgnt\n",
    "from utils.evaluate import evaluate, Accuracy\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "import random\n",
    "import inspect\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "torch.set_grad_enabled(False);\n",
    "\n",
    "from timm.models.vision_transformer import VisionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "897f4cb8-0962-4167-9cd0-f317eb06131b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageNetDataset\n",
       "    Number of datapoints: 50000\n",
       "    Root location: /scratch_shared/primmere/ILSVRC/Data/CLS-LOC/val\n",
       "    Compose(\n",
       "    ToTensor()\n",
       "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"/scratch_shared/primmere/ILSVRC/Data/CLS-LOC\"\n",
    "imagenet = imgnt.ImageNet(dataset_path, 1)\n",
    "val = imagenet.get_valid_set()\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = timm.create_model('deit_tiny_patch16_224.fb_in1k', pretrained=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "next(model.parameters()).is_cuda \n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b0518c-d361-43c7-b2b8-13b433b103ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices = random.sample(range(50_000),1000)\n",
    "#indices = imgnt.get_sample_indices_for_class(val, list(range(100)), 10_000, device)\n",
    "val_small = imgnt.ImageNetSubset(val,indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4be03a-10ee-4fea-8183-014745a363d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_small,8, pin_memory=True)\n",
    "img, label = next(iter(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368e012-d3c0-473c-9a70-f3b9d5585adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5161344a-a8c2-4c5a-be4d-44f102d3433e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 224, 224])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = img.cuda()\n",
    "img.device\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc05e4-4c17-42c8-b164-a64263d2e98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89f4b4a8-0bee-4397-83f9-a4160414609d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 55,  61, 428, 296, 769, 349, 728, 766])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90929e17-04ac-48a0-a360-ea5e566bf8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 64,  61, 756, 296, 769, 348, 728, 766], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model(img), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdcbd0f0-2615-4ca2-a38e-80c49e4a37a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 125/125 [00:08<00:00, 14.52batch/s]\n"
     ]
    }
   ],
   "source": [
    "acc = Accuracy()\n",
    "results = evaluate(model, val_loader, acc, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34e3c862-67f4-411a-bbdb-e0bf5aaae452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 2 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "0.724\n"
     ]
    }
   ],
   "source": [
    "print(results['confusion_matrix'])\n",
    "print(results['total_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "798827e9-607a-4e16-af5e-efa71ef02dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def __init__(\n",
      "            self,\n",
      "            img_size: Union[int, Tuple[int, int]] = 224,\n",
      "            patch_size: Union[int, Tuple[int, int]] = 16,\n",
      "            in_chans: int = 3,\n",
      "            num_classes: int = 1000,\n",
      "            global_pool: Literal['', 'avg', 'avgmax', 'max', 'token', 'map'] = 'token',\n",
      "            embed_dim: int = 768,\n",
      "            depth: int = 12,\n",
      "            num_heads: int = 12,\n",
      "            mlp_ratio: float = 4.,\n",
      "            qkv_bias: bool = True,\n",
      "            qk_norm: bool = False,\n",
      "            proj_bias: bool = True,\n",
      "            init_values: Optional[float] = None,\n",
      "            class_token: bool = True,\n",
      "            pos_embed: str = 'learn',\n",
      "            no_embed_class: bool = False,\n",
      "            reg_tokens: int = 0,\n",
      "            pre_norm: bool = False,\n",
      "            final_norm: bool = True,\n",
      "            fc_norm: Optional[bool] = None,\n",
      "            dynamic_img_size: bool = False,\n",
      "            dynamic_img_pad: bool = False,\n",
      "            drop_rate: float = 0.,\n",
      "            pos_drop_rate: float = 0.,\n",
      "            patch_drop_rate: float = 0.,\n",
      "            proj_drop_rate: float = 0.,\n",
      "            attn_drop_rate: float = 0.,\n",
      "            drop_path_rate: float = 0.,\n",
      "            weight_init: Literal['skip', 'jax', 'jax_nlhb', 'moco', ''] = '',\n",
      "            fix_init: bool = False,\n",
      "            embed_layer: Callable = PatchEmbed,\n",
      "            embed_norm_layer: Optional[LayerType] = None,\n",
      "            norm_layer: Optional[LayerType] = None,\n",
      "            act_layer: Optional[LayerType] = None,\n",
      "            block_fn: Type[nn.Module] = Block,\n",
      "            mlp_layer: Type[nn.Module] = Mlp,\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            img_size: Input image size.\n",
      "            patch_size: Patch size.\n",
      "            in_chans: Number of image input channels.\n",
      "            num_classes: Number of classes for classification head.\n",
      "            global_pool: Type of global pooling for final sequence (default: 'token').\n",
      "            embed_dim: Transformer embedding dimension.\n",
      "            depth: Depth of transformer.\n",
      "            num_heads: Number of attention heads.\n",
      "            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n",
      "            qkv_bias: Enable bias for qkv projections if True.\n",
      "            init_values: Layer-scale init values (layer-scale enabled if not None).\n",
      "            class_token: Use class token.\n",
      "            no_embed_class: Don't include position embeddings for class (or reg) tokens.\n",
      "            reg_tokens: Number of register tokens.\n",
      "            pre_norm: Enable norm after embeddings, before transformer blocks (standard in CLIP ViT).\n",
      "            final_norm: Enable norm after transformer blocks, before head (standard in most ViT).\n",
      "            fc_norm: Move final norm after pool (instead of before), if None, enabled when global_pool == 'avg'.\n",
      "            drop_rate: Head dropout rate.\n",
      "            pos_drop_rate: Position embedding dropout rate.\n",
      "            attn_drop_rate: Attention dropout rate.\n",
      "            drop_path_rate: Stochastic depth rate.\n",
      "            weight_init: Weight initialization scheme.\n",
      "            fix_init: Apply weight initialization fix (scaling w/ layer index).\n",
      "            embed_layer: Patch embedding layer.\n",
      "            embed_norm_layer: Normalization layer to use / override in patch embed module.\n",
      "            norm_layer: Normalization layer.\n",
      "            act_layer: MLP activation layer.\n",
      "            block_fn: Transformer block layer.\n",
      "        \"\"\"\n",
      "        super().__init__()\n",
      "        assert global_pool in ('', 'avg', 'avgmax', 'max', 'token', 'map')\n",
      "        assert class_token or global_pool != 'token'\n",
      "        assert pos_embed in ('', 'none', 'learn')\n",
      "        use_fc_norm = global_pool in ('avg', 'avgmax', 'max') if fc_norm is None else fc_norm\n",
      "        norm_layer = get_norm_layer(norm_layer) or partial(nn.LayerNorm, eps=1e-6)\n",
      "        embed_norm_layer = get_norm_layer(embed_norm_layer)\n",
      "        act_layer = get_act_layer(act_layer) or nn.GELU\n",
      "\n",
      "        self.num_classes = num_classes\n",
      "        self.global_pool = global_pool\n",
      "        self.num_features = self.head_hidden_size = self.embed_dim = embed_dim  # for consistency with other models\n",
      "        self.num_prefix_tokens = 1 if class_token else 0\n",
      "        self.num_prefix_tokens += reg_tokens\n",
      "        self.num_reg_tokens = reg_tokens\n",
      "        self.has_class_token = class_token\n",
      "        self.no_embed_class = no_embed_class  # don't embed prefix positions (includes reg)\n",
      "        self.dynamic_img_size = dynamic_img_size\n",
      "        self.grad_checkpointing = False\n",
      "\n",
      "        embed_args = {}\n",
      "        if dynamic_img_size:\n",
      "            # flatten deferred until after pos embed\n",
      "            embed_args.update(dict(strict_img_size=False, output_fmt='NHWC'))\n",
      "        if embed_norm_layer is not None:\n",
      "            embed_args['norm_layer'] = embed_norm_layer\n",
      "        self.patch_embed = embed_layer(\n",
      "            img_size=img_size,\n",
      "            patch_size=patch_size,\n",
      "            in_chans=in_chans,\n",
      "            embed_dim=embed_dim,\n",
      "            bias=not pre_norm,  # disable bias if pre-norm is used (e.g. CLIP)\n",
      "            dynamic_img_pad=dynamic_img_pad,\n",
      "            **embed_args,\n",
      "        )\n",
      "        num_patches = self.patch_embed.num_patches\n",
      "        reduction = self.patch_embed.feat_ratio() if hasattr(self.patch_embed, 'feat_ratio') else patch_size\n",
      "\n",
      "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if class_token else None\n",
      "        self.reg_token = nn.Parameter(torch.zeros(1, reg_tokens, embed_dim)) if reg_tokens else None\n",
      "        embed_len = num_patches if no_embed_class else num_patches + self.num_prefix_tokens\n",
      "        if not pos_embed or pos_embed == 'none':\n",
      "            self.pos_embed = None\n",
      "        else:\n",
      "            self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n",
      "        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n",
      "        if patch_drop_rate > 0:\n",
      "            self.patch_drop = PatchDropout(\n",
      "                patch_drop_rate,\n",
      "                num_prefix_tokens=self.num_prefix_tokens,\n",
      "            )\n",
      "        else:\n",
      "            self.patch_drop = nn.Identity()\n",
      "        self.norm_pre = norm_layer(embed_dim) if pre_norm else nn.Identity()\n",
      "\n",
      "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
      "        self.blocks = nn.Sequential(*[\n",
      "            block_fn(\n",
      "                dim=embed_dim,\n",
      "                num_heads=num_heads,\n",
      "                mlp_ratio=mlp_ratio,\n",
      "                qkv_bias=qkv_bias,\n",
      "                qk_norm=qk_norm,\n",
      "                proj_bias=proj_bias,\n",
      "                init_values=init_values,\n",
      "                proj_drop=proj_drop_rate,\n",
      "                attn_drop=attn_drop_rate,\n",
      "                drop_path=dpr[i],\n",
      "                norm_layer=norm_layer,\n",
      "                act_layer=act_layer,\n",
      "                mlp_layer=mlp_layer,\n",
      "            )\n",
      "            for i in range(depth)])\n",
      "        self.feature_info = [\n",
      "            dict(module=f'blocks.{i}', num_chs=embed_dim, reduction=reduction) for i in range(depth)]\n",
      "        self.norm = norm_layer(embed_dim) if final_norm and not use_fc_norm else nn.Identity()\n",
      "\n",
      "        # Classifier Head\n",
      "        if global_pool == 'map':\n",
      "            self.attn_pool = AttentionPoolLatent(\n",
      "                self.embed_dim,\n",
      "                num_heads=num_heads,\n",
      "                mlp_ratio=mlp_ratio,\n",
      "                norm_layer=norm_layer,\n",
      "                act_layer=act_layer,\n",
      "            )\n",
      "        else:\n",
      "            self.attn_pool = None\n",
      "        self.fc_norm = norm_layer(embed_dim) if final_norm and use_fc_norm else nn.Identity()\n",
      "        self.head_drop = nn.Dropout(drop_rate)\n",
      "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
      "\n",
      "        if weight_init != 'skip':\n",
      "            self.init_weights(weight_init)\n",
      "        if fix_init:\n",
      "            self.fix_init_weight()\n",
      "\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        x = self.forward_features(x)\n",
      "        x = self.forward_head(x)\n",
      "        return x\n",
      "\n",
      "/home/primmere/.conda/envs/dfr2/lib/python3.10/site-packages/timm/models/vision_transformer.py\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(VisionTransformer.__init__))  # constructor w/ blocks & heads\n",
    "print(inspect.getsource(VisionTransformer.forward))   # forward pass\n",
    "print(inspect.getsourcefile(VisionTransformer))       # file path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfr2",
   "language": "python",
   "name": "dfr2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
